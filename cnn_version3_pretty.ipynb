{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required package\n",
        "!pip install pretty_midi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fVqed0hor0o",
        "outputId": "3e34ab75-cd41-4fb8-cff9-fd8fae732ed3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/5.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/5.6 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (2.0.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido>=1.1.16->pretty_midi) (25.0)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=49d5e3ada18cfad702d5b26e68be816b66b3f11df2eefccf4baf6948f08dcb93\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi\n",
            "Successfully installed mido-1.3.3 pretty_midi-0.2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from google.colab import drive\n",
        "import random\n",
        "\n",
        "# Temporarily depending on performance- NEW\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "metadata": {
        "id": "Lja03CWIqPbK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "base_path = '/content/drive/MyDrive/NEW PROJECT LIST/DL COURSE SUMMER/Group Project/Composer_Dataset/NN_midi_files_extended'\n",
        "train_path = os.path.join(base_path, 'train')\n",
        "test_path = os.path.join(base_path, 'test')\n",
        "composers = ['bach', 'bartok', 'chopin', 'mozart']\n",
        "composer_to_idx = {composer: idx for idx, composer in enumerate(composers)}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFBthyZ2qMN-",
        "outputId": "bfebe3bf-bfa5-40f2-fef5-a7a8d94bb7ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "class MidiDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        midi_file = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load and preprocess MIDI\n",
        "        try:\n",
        "            midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
        "            piano_roll = midi_data.get_piano_roll(fs=100)  # Shape: (128, time_steps)\n",
        "\n",
        "            # Transpose to (time_steps, 128) if needed\n",
        "            piano_roll = piano_roll.T  # Shape: (time_steps, 128)\n",
        "\n",
        "            # Normalize\n",
        "            piano_roll = piano_roll / 127.0\n",
        "\n",
        "            # Pad or truncate to fixed time length (1000)\n",
        "            target_length = 1000\n",
        "            if piano_roll.shape[0] < target_length:\n",
        "                pad_width = ((0, target_length - piano_roll.shape[0]), (0, 0))\n",
        "                piano_roll = np.pad(piano_roll, pad_width, mode='constant')\n",
        "            else:\n",
        "                piano_roll = piano_roll[:target_length, :]\n",
        "\n",
        "            # Ensure pitch dimension is exactly 128\n",
        "            if piano_roll.shape[1] != 128:\n",
        "                if piano_roll.shape[1] < 128:\n",
        "                    pad_width = ((0, 0), (0, 128 - piano_roll.shape[1]))\n",
        "                    piano_roll = np.pad(piano_roll, pad_width, mode='constant')\n",
        "                else:\n",
        "                    piano_roll = piano_roll[:, :128]\n",
        "\n",
        "            # Add channel dimension: (1, time_steps, pitches)\n",
        "            piano_roll = np.expand_dims(piano_roll, axis=0)  # Shape: (1, 1000, 128)\n",
        "\n",
        "            # Apply data augmentation\n",
        "            if self.transform:\n",
        "                piano_roll = self.transform(piano_roll)\n",
        "\n",
        "            return torch.FloatTensor(piano_roll), torch.LongTensor([label])[0]\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {midi_file}: {e}\")\n",
        "            return torch.zeros((1, 1000, 128)), torch.LongTensor([label])[0]"
      ],
      "metadata": {
        "id": "LlctCELeqJDS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation functions\n",
        "def time_shift(piano_roll, max_shift=75):  # Increased max_shift\n",
        "    shift = random.randint(-max_shift, max_shift)\n",
        "    shifted = np.roll(piano_roll, shift, axis=1)  # Shift along time axis\n",
        "    if shift > 0:\n",
        "        shifted[:, :shift, :] = 0  # Zero-pad the start\n",
        "    elif shift < 0:\n",
        "        shifted[:, shift:, :] = 0  # Zero-pad the end\n",
        "    return shifted\n",
        "\n",
        "def pitch_shift(piano_roll, max_shift=10):  # Increased max_shift\n",
        "    shift = random.randint(-max_shift, max_shift)\n",
        "    shifted = np.roll(piano_roll, shift, axis=2)  # Shift along pitch axis\n",
        "    if shift > 0:\n",
        "        shifted[:, :, :shift] = 0  # Zero-pad the start\n",
        "    elif shift < 0:\n",
        "        shifted[:, :, shift:] = 0  # Zero-pad the end\n",
        "    return shifted\n",
        "\n",
        "def add_noise(piano_roll, noise_factor=0.1):  # Increased noise_factor\n",
        "    noise = np.random.normal(0, noise_factor, piano_roll.shape)\n",
        "    return np.clip(piano_roll + noise, 0, 1)\n",
        "\n",
        "def tempo_variation(piano_roll, factor=0.2):  # New transformation\n",
        "    scale = 1 + random.uniform(-factor, factor)\n",
        "    time_steps = piano_roll.shape[1]\n",
        "    new_time_steps = int(time_steps * scale)\n",
        "    if new_time_steps < 1:\n",
        "        new_time_steps = 1\n",
        "    # Interpolate along the time axis for each pitch\n",
        "    rescaled = np.zeros_like(piano_roll)  # Shape: (1, 1000, 128)\n",
        "    for i in range(piano_roll.shape[2]):  # Iterate over pitches\n",
        "        interpolated = np.interp(\n",
        "            np.linspace(0, time_steps, new_time_steps),\n",
        "            np.arange(time_steps),\n",
        "            piano_roll[0, :, i]\n",
        "        )\n",
        "        # Resize to original time_steps (1000) using interpolation or truncation\n",
        "        if new_time_steps > time_steps:\n",
        "            rescaled[0, :, i] = np.interp(\n",
        "                np.linspace(0, new_time_steps, time_steps),\n",
        "                np.arange(new_time_steps),\n",
        "                interpolated\n",
        "            )\n",
        "        else:\n",
        "            rescaled[0, :new_time_steps, i] = interpolated\n",
        "            rescaled[0, new_time_steps:, i] = 0  # Zero-pad if shorter\n",
        "    return rescaled\n",
        "\n",
        "def augment_data(piano_roll):\n",
        "    if random.random() > 0.3:  # Increased application frequency\n",
        "        piano_roll = time_shift(piano_roll)\n",
        "    if random.random() > 0.3:\n",
        "        piano_roll = pitch_shift(piano_roll)\n",
        "    if random.random() > 0.3:\n",
        "        piano_roll = add_noise(piano_roll)\n",
        "    if random.random() > 0.5:\n",
        "        piano_roll = tempo_variation(piano_roll)\n",
        "    return piano_roll"
      ],
      "metadata": {
        "id": "cEGKbH3MqAmb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MIDI files\n",
        "def load_midi_files(data_path):\n",
        "    file_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for composer in composers:\n",
        "        composer_path = os.path.join(data_path, composer)\n",
        "        for file in os.listdir(composer_path):\n",
        "            if file.endswith('.mid') or file.endswith('.midi'):\n",
        "                file_paths.append(os.path.join(composer_path, file))\n",
        "                labels.append(composer_to_idx[composer])\n",
        "\n",
        "    return file_paths, labels"
      ],
      "metadata": {
        "id": "Zu4hAGC_p9sb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Model\n",
        "class ComposerCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(ComposerCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Calculate the size of flattened features\n",
        "        self.flatten_size = 256 * (1000 // 8) * (128 // 8)\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self.flatten_size, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.7),  # Increased dropout\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JUsbTq0vp5t1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_files, train_labels = load_midi_files(train_path)\n",
        "test_files, test_labels = load_midi_files(test_path)"
      ],
      "metadata": {
        "id": "mqhujUDHp1i-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split training data into train and validation sets\n",
        "train_files, val_files, train_labels, val_labels = train_test_split(train_files, train_labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Cjkuz-tBpxiC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = MidiDataset(train_files, train_labels, transform=augment_data)\n",
        "val_dataset = MidiDataset(val_files, val_labels)\n",
        "test_dataset = MidiDataset(test_files, test_labels)"
      ],
      "metadata": {
        "id": "MbZ2EMTapvky"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Reduced batch size\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "NKlxmDkWptlf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ComposerCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Reduced learning rate\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Learning rate scheduler"
      ],
      "metadata": {
        "id": "NCrq3Czoprwk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with early stopping\n",
        "num_epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "patience, trials = 20, 0  # was 10, but this was underperforming so I adjusted it to 20.\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        trials = 0\n",
        "    else:\n",
        "        trials += 1\n",
        "        if trials >= patience:\n",
        "            print(f'Early stopping triggered after epoch {epoch+1}')\n",
        "            break\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoCugOz_pmkr",
        "outputId": "b86d5417-4d34-427a-a1a4-9cce2bce2075"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 21.0012, Val Loss: 9.8513\n",
            "Epoch 2/50, Train Loss: 17.9280, Val Loss: 16.8986\n",
            "Epoch 3/50, Train Loss: 13.2961, Val Loss: 6.4396\n",
            "Epoch 4/50, Train Loss: 13.9754, Val Loss: 15.4359\n",
            "Epoch 5/50, Train Loss: 12.5262, Val Loss: 18.7953\n",
            "Epoch 6/50, Train Loss: 11.8264, Val Loss: 29.6960\n",
            "Epoch 7/50, Train Loss: 9.3842, Val Loss: 22.7654\n",
            "Epoch 8/50, Train Loss: 11.7012, Val Loss: 36.0533\n",
            "Epoch 9/50, Train Loss: 8.8249, Val Loss: 24.9890\n",
            "Epoch 10/50, Train Loss: 6.1273, Val Loss: 21.4801\n",
            "Epoch 11/50, Train Loss: 6.2106, Val Loss: 26.9299\n",
            "Epoch 12/50, Train Loss: 7.2173, Val Loss: 37.7424\n",
            "Epoch 13/50, Train Loss: 5.0696, Val Loss: 33.1504\n",
            "Epoch 14/50, Train Loss: 3.6833, Val Loss: 43.2616\n",
            "Epoch 15/50, Train Loss: 3.9319, Val Loss: 47.8091\n",
            "Epoch 16/50, Train Loss: 2.7175, Val Loss: 47.5252\n",
            "Epoch 17/50, Train Loss: 3.4549, Val Loss: 44.2628\n",
            "Epoch 18/50, Train Loss: 3.2459, Val Loss: 42.0771\n",
            "Epoch 19/50, Train Loss: 3.2057, Val Loss: 32.6244\n",
            "Epoch 20/50, Train Loss: 2.8620, Val Loss: 32.5151\n",
            "Epoch 21/50, Train Loss: 2.5460, Val Loss: 40.3740\n",
            "Epoch 22/50, Train Loss: 2.1819, Val Loss: 47.4560\n",
            "Epoch 23/50, Train Loss: 2.4254, Val Loss: 44.3446\n",
            "Early stopping triggered after epoch 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())"
      ],
      "metadata": {
        "id": "tQkbHohNpkOS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and print classification report\n",
        "print('\\nClassification Report:\\n')\n",
        "print(classification_report(all_labels, all_preds, target_names=composers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poxC5yOHpiPB",
        "outputId": "21254434-c662-44f8-ce7d-5d002c434e17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        bach       1.00      1.00      1.00         4\n",
            "      bartok       0.67      1.00      0.80         4\n",
            "      chopin       0.50      0.50      0.50         4\n",
            "      mozart       0.50      0.25      0.33         4\n",
            "\n",
            "    accuracy                           0.69        16\n",
            "   macro avg       0.67      0.69      0.66        16\n",
            "weighted avg       0.67      0.69      0.66        16\n",
            "\n"
          ]
        }
      ]
    }
  ]
}